{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1sQpXJWk_vuFuvN_0i0hS83Z8QAgiQJlv",
      "authorship_tag": "ABX9TyOo2q0MzuBcNUPszT7ch5gJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RArunn/Intent-Identification-Detection/blob/main/hicodet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Drive Mount and Dataset Download\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IjNKh7bioYTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and GPU Configuration\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import gc\n",
        "import tarfile\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import warnings\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# GPU Optimizations\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False"
      ],
      "metadata": {
        "id": "4sgdfVXIo4mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Dataset Setup\n",
        "def setup_hicodet_test2015(base_path=\"/content/drive/MyDrive/hicodet\"):\n",
        "    \"\"\"Setup for test2015 dataset\"\"\"\n",
        "    tar_filename = os.path.join(base_path, \"hico_20160224_det.tar.gz\")\n",
        "\n",
        "    if not os.path.exists(tar_filename):\n",
        "        raise FileNotFoundError(f\"Dataset not found at {tar_filename}\")\n",
        "\n",
        "    test_dir = \"images/test2015\"\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    test_count = len(os.listdir(test_dir)) if os.path.exists(test_dir) else 0\n",
        "\n",
        "    if test_count < 9600:\n",
        "        with tarfile.open(tar_filename, 'r:gz') as tar_ref:\n",
        "            for member in tqdm(tar_ref.getmembers(), desc=\"Extracting test2015\"):\n",
        "                if \"images/test2015/\" in member.name and member.isfile() and member.name.endswith('.jpg'):\n",
        "                    filename = os.path.basename(member.name)\n",
        "                    tar_ref.extract(member, \"temp/\")\n",
        "                    shutil.move(f\"temp/{member.name}\", f\"{test_dir}/{filename}\")\n",
        "\n",
        "        shutil.rmtree(\"temp\", ignore_errors=True)\n",
        "        test_count = len(os.listdir(test_dir))\n",
        "\n",
        "    return test_dir"
      ],
      "metadata": {
        "id": "tndeYRiVo10n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Model Initialization\n",
        "def init_model():\n",
        "    \"\"\"Initialize model and processor\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
        "\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "PGixwkYuoyCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Dataset Class\n",
        "class HicoDetDataset(Dataset):\n",
        "    def __init__(self, image_dir, max_images=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "\n",
        "        if max_images:\n",
        "            self.image_files = self.image_files[:max_images]\n",
        "\n",
        "        # Pre-compute image IDs\n",
        "        self.image_ids = []\n",
        "        for img_file in self.image_files:\n",
        "            base_id = img_file.replace('.jpg', '')\n",
        "            if base_id.startswith('HICO_'):\n",
        "                self.image_ids.append(base_id)\n",
        "            else:\n",
        "                self.image_ids.append(f\"HICO_test2015_{base_id.zfill(8)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file = self.image_files[idx]\n",
        "        image_id = self.image_ids[idx]\n",
        "        image_path = os.path.join(self.image_dir, img_file)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            original_size = image.size\n",
        "\n",
        "            if max(original_size) > 448:\n",
        "                ratio = 448 / max(original_size)\n",
        "                new_size = (int(original_size[0] * ratio), int(original_size[1] * ratio))\n",
        "                image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "            return {\n",
        "                'image': image,\n",
        "                'image_id': image_id,\n",
        "                'scale': np.array([original_size[0] / image.size[0], original_size[1] / image.size[1]], dtype=np.float32),\n",
        "                'original_size': original_size,\n",
        "                'filename': img_file\n",
        "            }\n",
        "        except:\n",
        "            return None"
      ],
      "metadata": {
        "id": "zsx7LMKoouI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Coordinate Extraction\n",
        "class CoordinateExtractor:\n",
        "    PATTERN_WITH_CONF = re.compile(\n",
        "        r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+confidence\\s+([0-9]*\\.?[0-9]+)',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    PATTERN_NO_CONF = re.compile(\n",
        "        r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    BLACKLIST = {'object', 'thing', 'item'}\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_detections_vectorized(text, image_data):\n",
        "        \"\"\"Vectorized coordinate extraction\"\"\"\n",
        "        results = []\n",
        "\n",
        "        matches = CoordinateExtractor.PATTERN_WITH_CONF.findall(text.lower())\n",
        "        has_confidence = True\n",
        "\n",
        "        if not matches:\n",
        "            matches = CoordinateExtractor.PATTERN_NO_CONF.findall(text.lower())\n",
        "            has_confidence = False\n",
        "\n",
        "        if not matches:\n",
        "            return results\n",
        "\n",
        "        scale = image_data['scale']\n",
        "        width, height = image_data['original_size']\n",
        "\n",
        "        for match in matches:\n",
        "            try:\n",
        "                coords = [int(x) for x in match[:4]]\n",
        "                action = match[4].strip()\n",
        "                obj = match[5].strip()\n",
        "                obj_coords = [int(x) for x in match[6:10]]\n",
        "\n",
        "                if (len(obj) < 2 or obj in CoordinateExtractor.BLACKLIST or\n",
        "                    any(c <= 0 for c in coords[2:4]) or any(c <= 0 for c in obj_coords[2:4])):\n",
        "                    continue\n",
        "\n",
        "                person_coords = np.array(coords, dtype=np.float32)\n",
        "                object_coords = np.array(obj_coords, dtype=np.float32)\n",
        "\n",
        "                person_coords[[0, 2]] *= scale[0]\n",
        "                person_coords[[1, 3]] *= scale[1]\n",
        "                object_coords[[0, 2]] *= scale[0]\n",
        "                object_coords[[1, 3]] *= scale[1]\n",
        "\n",
        "                person_box = [\n",
        "                    max(0, min(width, int(person_coords[0]))),\n",
        "                    max(0, min(height, int(person_coords[1]))),\n",
        "                    max(0, min(width, int(person_coords[0] + person_coords[2]))),\n",
        "                    max(0, min(height, int(person_coords[1] + person_coords[3])))\n",
        "                ]\n",
        "\n",
        "                object_box = [\n",
        "                    max(0, min(width, int(object_coords[0]))),\n",
        "                    max(0, min(height, int(object_coords[1]))),\n",
        "                    max(0, min(width, int(object_coords[0] + object_coords[2]))),\n",
        "                    max(0, min(height, int(object_coords[1] + object_coords[3])))\n",
        "                ]\n",
        "\n",
        "                if (person_box[2] > person_box[0] and person_box[3] > person_box[1] and\n",
        "                    object_box[2] > object_box[0] and object_box[3] > object_box[1]):\n",
        "\n",
        "                    confidence = float(match[10]) if has_confidence and len(match) > 10 else 0.8\n",
        "                    confidence = max(0.1, min(1.0, confidence))\n",
        "\n",
        "                    results.append({\n",
        "                        'image_id': image_data['image_id'],\n",
        "                        'person_box': person_box,\n",
        "                        'object_box': object_box,\n",
        "                        'action': action,\n",
        "                        'object': obj,\n",
        "                        'confidence': confidence\n",
        "                    })\n",
        "\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "66h-MhcNorPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Batch Inference\n",
        "def batch_inference(batch_data, model, processor):\n",
        "    \"\"\"Optimized batch inference\"\"\"\n",
        "    valid_batch = [item for item in batch_data if item is not None]\n",
        "    if not valid_batch:\n",
        "        return []\n",
        "\n",
        "    prompt = \"\"\"Analyze this image for human-object interactions. For each interaction you find, provide:\n",
        "\n",
        "1. What the person is doing\n",
        "2. What object they're interacting with\n",
        "3. Location of the person as [x, y, width, height]\n",
        "4. Location of the object as [x, y, width, height]\n",
        "\n",
        "Format your response as:\n",
        "Person at [x,y,w,h] doing ACTION with OBJECT at [x,y,w,h]\n",
        "\n",
        "Only include clear, visible interactions. Maximum 10 interactions per image.\"\"\"\n",
        "\n",
        "    images = [item['image'] for item in valid_batch]\n",
        "\n",
        "    messages_list = []\n",
        "    for image in images:\n",
        "        messages_list.append([{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }])\n",
        "\n",
        "    try:\n",
        "        texts = [processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "                 for msgs in messages_list]\n",
        "\n",
        "        inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            max_length=1024,\n",
        "            truncation=True\n",
        "        ).to(model.device, non_blocking=True)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                output_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=200,\n",
        "                    do_sample=False,\n",
        "                    use_cache=True,\n",
        "                    pad_token_id=processor.tokenizer.eos_token_id,\n",
        "                    num_beams=1\n",
        "                )\n",
        "\n",
        "        generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "        responses = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        all_results = []\n",
        "        for data, response in zip(valid_batch, responses):\n",
        "            results = CoordinateExtractor.extract_detections_vectorized(response, data)\n",
        "            all_results.extend(results)\n",
        "\n",
        "        del inputs, output_ids, generated_ids\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        return []"
      ],
      "metadata": {
        "id": "ambcna7Wom41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Processing Pipeline\n",
        "def process_hicodet(model, processor, image_dir,\n",
        "                   output_file=\"hicodet_output.json\",\n",
        "                   batch_size=32, max_images=None, save_interval=2000):\n",
        "    \"\"\"Processing pipeline with optimizations\"\"\"\n",
        "\n",
        "    dataset = HicoDetDataset(image_dir, max_images)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=12,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda x: [item for item in x if item is not None],\n",
        "        persistent_workers=True,\n",
        "        prefetch_factor=3\n",
        "    )\n",
        "\n",
        "    all_detections = []\n",
        "    processed_count = 0\n",
        "    images_with_detections = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=\"Processing\")):\n",
        "        if not batch_data:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            batch_results = batch_inference(batch_data, model, processor)\n",
        "\n",
        "            if batch_results:\n",
        "                batch_image_ids = set(r['image_id'] for r in batch_results)\n",
        "                images_with_detections += len(batch_image_ids)\n",
        "\n",
        "            all_detections.extend(batch_results)\n",
        "            processed_count += len(batch_data)\n",
        "\n",
        "            if processed_count % save_interval == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                rate = processed_count / elapsed\n",
        "\n",
        "                checkpoint_data = {\n",
        "                    'detections': all_detections,\n",
        "                    'metadata': {\n",
        "                        'processed_count': processed_count,\n",
        "                        'detection_count': len(all_detections),\n",
        "                        'images_with_detections': images_with_detections,\n",
        "                        'elapsed_time': elapsed,\n",
        "                        'images_per_second': rate,\n",
        "                        'estimated_completion': elapsed * (len(dataset) / processed_count - 1) / 3600\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                checkpoint_file = f\"{output_file}.checkpoint_{processed_count}.json\"\n",
        "                with open(checkpoint_file, 'w') as f:\n",
        "                    json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    images_per_second = processed_count / elapsed_time\n",
        "\n",
        "    final_output = {\n",
        "        'detections': all_detections,\n",
        "        'metadata': {\n",
        "            'total_images_processed': processed_count,\n",
        "            'total_detections': len(all_detections),\n",
        "            'images_with_detections': images_with_detections,\n",
        "            'detection_rate': f\"{images_with_detections/processed_count*100:.1f}%\" if processed_count > 0 else \"0%\",\n",
        "            'processing_time_hours': elapsed_time / 3600,\n",
        "            'images_per_second': images_per_second,\n",
        "            'batch_size': batch_size,\n",
        "            'vocab_restrictions': 'None - Full Qwen2.5-VL vocabulary',\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(final_output, f, indent=2)\n",
        "\n",
        "    return all_detections"
      ],
      "metadata": {
        "id": "lAwdL8dgojKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test Processing\n",
        "def run_test():\n",
        "    \"\"\"Test processing\"\"\"\n",
        "    test_dir = setup_hicodet_test2015()\n",
        "    model, processor = init_model()\n",
        "\n",
        "    results = process_hicodet(\n",
        "        model, processor, test_dir,\n",
        "        output_file=\"hicodet_output_test.json\",\n",
        "        batch_size=24,\n",
        "        max_images=100,\n",
        "        save_interval=50\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "nlBVHsGmofUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Full Processing\n",
        "def run_full_processing():\n",
        "    \"\"\"Full dataset processing\"\"\"\n",
        "    test_dir = setup_hicodet_test2015()\n",
        "    model, processor = init_model()\n",
        "\n",
        "    batch_size = 32\n",
        "\n",
        "    results = process_hicodet(\n",
        "        model, processor, test_dir,\n",
        "        output_file=\"hicodet_output_full.json\",\n",
        "        batch_size=batch_size,\n",
        "        max_images=None,\n",
        "        save_interval=2000\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        actions = {}\n",
        "        objects = {}\n",
        "        for result in results:\n",
        "            action = result['action']\n",
        "            obj = result['object']\n",
        "            actions[action] = actions.get(action, 0) + 1\n",
        "            objects[obj] = objects.get(obj, 0) + 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "7MgcyFaCodFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Execute\n",
        "if __name__ == \"__main__\":\n",
        "    # Run test first\n",
        "    # test_results = run_test()\n",
        "\n",
        "    # Run full processing\n",
        "    full_results = run_full_processing()"
      ],
      "metadata": {
        "id": "a3Cho2XEoalk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}