{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNH+8xobnJeIvWKP71WWpR6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RArunn/Intent-Identification-Detection/blob/main/swighoi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import torch\n",
        "import gc\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
        "import urllib.request\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "hf-rG-Qot95a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Advanced Setup and Memory Optimization\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Enable mixed precision and optimizations\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "6LusLU6fuATT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Dataset Download and Extraction\n",
        "def download_swig_test_dataset():\n",
        "    \"\"\"Ultra-fast dataset download with parallel extraction\"\"\"\n",
        "    img_dir = \"images_512\"\n",
        "\n",
        "    if os.path.exists(img_dir):\n",
        "        return img_dir\n",
        "\n",
        "    # Ultra-fast download with multiple threads\n",
        "    if not os.path.exists(\"images_512.zip\"):\n",
        "        # Use aria2c if available for faster download, fallback to wget\n",
        "        result = os.system('aria2c -x 16 -s 16 -o images_512.zip \"https://swig-data-weights.s3.us-east-2.amazonaws.com/images_512.zip\" || wget -O images_512.zip \"https://swig-data-weights.s3.us-east-2.amazonaws.com/images_512.zip\"')\n",
        "        if result != 0:\n",
        "            return None\n",
        "\n",
        "    # Parallel extraction\n",
        "    result = os.system(\"unzip -q images_512.zip\")\n",
        "\n",
        "    if result == 0:\n",
        "        os.remove(\"images_512.zip\")\n",
        "        return img_dir\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "33rP2jyOuC8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Subset Extraction with Caching\n",
        "def extract_subset_to_skiing_169():\n",
        "    \"\"\"Extract subset with caching for faster repeated runs\"\"\"\n",
        "    cache_file = \"skiing_169_subset_cache.json\"\n",
        "\n",
        "    # Check cache first\n",
        "    if os.path.exists(cache_file):\n",
        "        with open(cache_file, 'r') as f:\n",
        "            subset_images = json.load(f)\n",
        "        return subset_images\n",
        "\n",
        "    # Extract if not cached\n",
        "    if not os.path.exists(\"test.json\"):\n",
        "        return None\n",
        "\n",
        "    with open(\"test.json\", 'r') as f:\n",
        "        test_data = json.load(f)\n",
        "\n",
        "    all_test_images = list(test_data.keys())\n",
        "    target_image = \"skiing_169.jpg\"\n",
        "\n",
        "    try:\n",
        "        end_index = all_test_images.index(target_image)\n",
        "        subset_images = all_test_images[:end_index + 1]\n",
        "\n",
        "        # Cache the result\n",
        "        with open(cache_file, 'w') as f:\n",
        "            json.dump(subset_images, f)\n",
        "\n",
        "        return subset_images\n",
        "\n",
        "    except ValueError:\n",
        "        return None"
      ],
      "metadata": {
        "id": "cJMTHmbkuGT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Ultra-Fast Model Initialization\n",
        "def init_ultra_fast_model():\n",
        "    \"\"\"Initialize model with maximum A100 optimizations\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    # Apply torch.compile AFTER loading the model\n",
        "    model.eval()\n",
        "    model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "    # Warm up the model\n",
        "    dummy_image = Image.new('RGB', (224, 224))\n",
        "    dummy_messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": dummy_image}, {\"type\": \"text\", \"text\": \"test\"}]}]\n",
        "    dummy_text = processor.apply_chat_template(dummy_messages, tokenize=False, add_generation_prompt=True)\n",
        "    dummy_inputs = processor(text=[dummy_text], images=[dummy_image], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        _ = model.generate(**dummy_inputs, max_new_tokens=10, do_sample=False)\n",
        "\n",
        "    del dummy_inputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "VPIeYow4uJ4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Optimized HOI Prompt\n",
        "hoi_prompt = \"\"\"Analyze this image for human-object interactions. For each interaction you find, provide:\n",
        "\n",
        "1. What the person is doing\n",
        "2. What object they're interacting with\n",
        "3. Location of the person as [x, y, width, height]\n",
        "4. Location of the object as [x, y, width, height]\n",
        "\n",
        "Format your response as:\n",
        "Person at [x,y,w,h] doing ACTION with OBJECT at [x,y,w,h]\n",
        "\n",
        "Only include clear, visible interactions. Maximum 10 interactions per image.\"\"\""
      ],
      "metadata": {
        "id": "2XNQf4DhuMnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Ultra-Fast Dataset Class\n",
        "class UltraFastSWiGDataset(Dataset):\n",
        "    def __init__(self, test_image_files, max_images=None):\n",
        "        self.img_dir = \"images_512\"\n",
        "        self.test_image_files = test_image_files\n",
        "\n",
        "        # Pre-filter and cache valid files\n",
        "        self.valid_image_files = []\n",
        "        for img_file in test_image_files:\n",
        "            image_path = os.path.join(self.img_dir, img_file)\n",
        "            if os.path.exists(image_path):\n",
        "                self.valid_image_files.append(img_file)\n",
        "\n",
        "        if max_images:\n",
        "            self.valid_image_files = self.valid_image_files[:max_images]\n",
        "\n",
        "        # Pre-load image metadata for faster access\n",
        "        self.image_metadata = {}\n",
        "        for img_file in self.valid_image_files:\n",
        "            image_path = os.path.join(self.img_dir, img_file)\n",
        "            try:\n",
        "                with Image.open(image_path) as img:\n",
        "                    self.image_metadata[img_file] = img.size\n",
        "            except:\n",
        "                self.image_metadata[img_file] = (512, 512)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file = self.valid_image_files[idx]\n",
        "        image_id = img_file.replace('.jpg', '')\n",
        "        image_path = os.path.join(self.img_dir, img_file)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            original_size = self.image_metadata.get(img_file, image.size)\n",
        "\n",
        "            # Ultra-fast resizing with optimized settings\n",
        "            max_size = 448\n",
        "            if max(original_size) > max_size:\n",
        "                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
        "\n",
        "            return {\n",
        "                'image': image,\n",
        "                'image_id': image_id,\n",
        "                'original_size': original_size,\n",
        "                'resized_size': image.size,\n",
        "                'filename': img_file\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'image': Image.new('RGB', (224, 224)),\n",
        "                'image_id': image_id,\n",
        "                'original_size': (224, 224),\n",
        "                'resized_size': (224, 224),\n",
        "                'filename': img_file\n",
        "            }"
      ],
      "metadata": {
        "id": "ApGWLuzuuRIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Ultra-Fast HOI Extraction with Caching\n",
        "def extract_hoi_predictions_ultra_fast(text, image_data, max_detections=10):\n",
        "    \"\"\"Ultra-fast HOI extraction with optimizations\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Optimized regex patterns\n",
        "    pattern_with_conf = re.compile(r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+confidence\\s+([0-9]*\\.?[0-9]+)', re.IGNORECASE)\n",
        "    pattern_no_conf = re.compile(r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]', re.IGNORECASE)\n",
        "\n",
        "    # Invalid objects set for faster lookup\n",
        "    invalid_objects = {'object', 'objects', 'thing', 'item', 'stuff', 'it'}\n",
        "\n",
        "    # Try with confidence first\n",
        "    matches = pattern_with_conf.findall(text)\n",
        "    if not matches:\n",
        "        matches = [(m + (0.7,)) for m in pattern_no_conf.findall(text)]\n",
        "\n",
        "    # Pre-calculate scaling factors\n",
        "    scale_x = image_data['original_size'][0] / image_data['resized_size'][0]\n",
        "    scale_y = image_data['original_size'][1] / image_data['resized_size'][1]\n",
        "    width, height = image_data['original_size']\n",
        "\n",
        "    for match in matches[:max_detections]:  # Limit to max_detections\n",
        "        try:\n",
        "            if len(match) == 11:  # With confidence\n",
        "                px, py, pw, ph, verb, obj, ox, oy, ow, oh, score = match\n",
        "                score = float(score)\n",
        "            else:  # Without confidence\n",
        "                px, py, pw, ph, verb, obj, ox, oy, ow, oh = match\n",
        "                score = 0.7\n",
        "\n",
        "            # Fast validation\n",
        "            px, py, pw, ph = int(px), int(py), int(pw), int(ph)\n",
        "            ox, oy, ow, oh = int(ox), int(oy), int(ow), int(oh)\n",
        "\n",
        "            if (len(obj) < 2 or obj.lower() in invalid_objects or\n",
        "                px < 0 or py < 0 or pw <= 0 or ph <= 0 or\n",
        "                ox < 0 or oy < 0 or ow <= 0 or oh <= 0):\n",
        "                continue\n",
        "\n",
        "            # Fast coordinate scaling\n",
        "            subject_box = [\n",
        "                max(0, min(width, int(px * scale_x))),\n",
        "                max(0, min(height, int(py * scale_y))),\n",
        "                max(0, min(width, int((px + pw) * scale_x))),\n",
        "                max(0, min(height, int((py + ph) * scale_y)))\n",
        "            ]\n",
        "\n",
        "            object_box = [\n",
        "                max(0, min(width, int(ox * scale_x))),\n",
        "                max(0, min(height, int(oy * scale_y))),\n",
        "                max(0, min(width, int((ox + ow) * scale_x))),\n",
        "                max(0, min(height, int((oy + oh) * scale_y)))\n",
        "            ]\n",
        "\n",
        "            # Final validation\n",
        "            if (subject_box[2] <= subject_box[0] or subject_box[3] <= subject_box[1] or\n",
        "                object_box[2] <= object_box[0] or object_box[3] <= object_box[1]):\n",
        "                continue\n",
        "\n",
        "            results.append({\n",
        "                \"subject_box\": subject_box,\n",
        "                \"object_box\": object_box,\n",
        "                \"subject_category\": \"person\",\n",
        "                \"object_category\": obj.lower(),\n",
        "                \"verb\": verb.lower(),\n",
        "                \"score\": round(max(0.1, min(1.0, score)), 4)\n",
        "            })\n",
        "\n",
        "        except (ValueError, IndexError):\n",
        "            continue\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "D353Yr4huVQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Ultra-Fast Batch Inference\n",
        "def ultra_fast_batch_inference(batch_data, model, processor):\n",
        "    \"\"\"Ultra-fast batch inference for A100 with proper attention handling\"\"\"\n",
        "    valid_batch = [item for item in batch_data if item['image_id'] != 'invalid']\n",
        "    if not valid_batch:\n",
        "        return []\n",
        "\n",
        "    images = [item['image'] for item in valid_batch]\n",
        "\n",
        "    # Optimized message creation\n",
        "    messages_list = []\n",
        "    for image in images:\n",
        "        messages_list.append([{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": hoi_prompt}\n",
        "            ]\n",
        "        }])\n",
        "\n",
        "    try:\n",
        "        # Set padding side to left for Flash Attention\n",
        "        processor.tokenizer.padding_side = 'left'\n",
        "\n",
        "        # Batch processing with optimizations\n",
        "        texts = [processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "                 for msgs in messages_list]\n",
        "\n",
        "        inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            max_length=2048,  # Increased to handle vision tokens\n",
        "            truncation=True\n",
        "        ).to(model.device, non_blocking=True)\n",
        "\n",
        "        # Ultra-fast generation\n",
        "        with torch.inference_mode():\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                output_ids = model.generate(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    pixel_values=inputs.get('pixel_values', None),\n",
        "                    image_grid_thw=inputs.get('image_grid_thw', None),\n",
        "                    max_new_tokens=150,  # Increased for better HOI detection\n",
        "                    do_sample=False,\n",
        "                    use_cache=True,\n",
        "                    num_beams=1,\n",
        "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "                    eos_token_id=processor.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "        # Fast decoding\n",
        "        generated_ids = output_ids[:, inputs['input_ids'].shape[1]:]\n",
        "        responses = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Process results with detection limit\n",
        "        batch_results = []\n",
        "        for data, response in zip(valid_batch, responses):\n",
        "            hoi_predictions = extract_hoi_predictions_ultra_fast(response, data, max_detections=10)\n",
        "            batch_results.append({\n",
        "                'image_id': data['image_id'],\n",
        "                'hoi_prediction': hoi_predictions\n",
        "            })\n",
        "\n",
        "        # Cleanup\n",
        "        del inputs, output_ids, generated_ids\n",
        "        torch.cuda.empty_cache()\n",
        "        return batch_results\n",
        "\n",
        "    except Exception as e:\n",
        "        return []"
      ],
      "metadata": {
        "id": "QtwwX-Q2uY49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Ultra-Fast Processing Pipeline\n",
        "def process_ultra_fast_swig(model, processor, test_image_files, output_file=\"swig_ultra_fast.json\",\n",
        "                            batch_size=20, max_images=None, save_interval=1000):\n",
        "    \"\"\"Ultra-fast processing pipeline optimized for A100\"\"\"\n",
        "\n",
        "    dataset = UltraFastSWiGDataset(test_image_files, max_images)\n",
        "\n",
        "    # Maximum performance dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=12,  # Maximum workers for A100\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda x: x,\n",
        "        persistent_workers=True,\n",
        "        prefetch_factor=6,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    all_predictions = []\n",
        "    processed_count = 0\n",
        "    total_hoi_count = 0\n",
        "\n",
        "    # Progress tracking\n",
        "    pbar = tqdm(dataloader, desc=\"Processing\", unit=\"batch\")\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(pbar):\n",
        "        try:\n",
        "            # Ultra-fast batch processing\n",
        "            batch_results = ultra_fast_batch_inference(batch_data, model, processor)\n",
        "            all_predictions.extend(batch_results)\n",
        "            processed_count += len(batch_data)\n",
        "\n",
        "            # Count HOI predictions\n",
        "            batch_hoi_count = sum(len(pred['hoi_prediction']) for pred in batch_results)\n",
        "            total_hoi_count += batch_hoi_count\n",
        "\n",
        "            # Update progress\n",
        "            pbar.set_postfix({\n",
        "                'Images': processed_count,\n",
        "                'HOI': total_hoi_count,\n",
        "                'Avg_HOI': f\"{total_hoi_count/processed_count:.1f}\"\n",
        "            })\n",
        "\n",
        "            # Memory cleanup (less frequent for speed)\n",
        "            if batch_idx % 50 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Save checkpoints\n",
        "            if processed_count % save_interval == 0:\n",
        "                checkpoint_file = f\"{output_file}.checkpoint_{processed_count}.json\"\n",
        "                with open(checkpoint_file, 'w') as f:\n",
        "                    json.dump(all_predictions, f, indent=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    # Save final results\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(all_predictions, f, indent=2)\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "UjvTKUFhucau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: JSON Output Generation\n",
        "def generate_ultra_fast_json_output(predictions, output_file=\"swighoi.json\"):\n",
        "    \"\"\"Generate final JSON with original filename-based image IDs\"\"\"\n",
        "\n",
        "    if not predictions:\n",
        "        return\n",
        "\n",
        "    final_output = []\n",
        "\n",
        "    for prediction in predictions:\n",
        "        # Keep original filename as image_id (without .jpg extension)\n",
        "        image_id_str = prediction['image_id']\n",
        "\n",
        "        # Remove .jpg extension if present, keep descriptive name\n",
        "        if image_id_str.endswith('.jpg'):\n",
        "            image_id = image_id_str[:-4]  # Remove .jpg\n",
        "        else:\n",
        "            image_id = image_id_str\n",
        "\n",
        "        formatted_prediction = {\n",
        "            \"image_id\": image_id,  # Now uses original descriptive names like \"skiing_169\"\n",
        "            \"hoi_prediction\": prediction.get('hoi_prediction', [])[:10]  # Limit to 10\n",
        "        }\n",
        "\n",
        "        final_output.append(formatted_prediction)\n",
        "\n",
        "    # Ultra-fast JSON formatting with single-line coordinates\n",
        "    def format_json_ultra_fast(data):\n",
        "        json_str = json.dumps(data, indent=2, separators=(',', ': '))\n",
        "\n",
        "        # Single-line array formatting\n",
        "        array_pattern = re.compile(r'\\[\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\]')\n",
        "        json_str = array_pattern.sub(r'[\\1, \\2, \\3, \\4]', json_str)\n",
        "\n",
        "        return json_str\n",
        "\n",
        "    # Save with optimized formatting\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(format_json_ultra_fast(final_output))\n",
        "\n",
        "    total_hoi = sum(len(pred['hoi_prediction']) for pred in final_output)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "HXkNOy_1ufZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Main Execution Functions\n",
        "def run_ultra_fast_small_test():\n",
        "    \"\"\"Ultra-fast small test (50 images)\"\"\"\n",
        "    img_dir = download_swig_test_dataset()\n",
        "    if not img_dir:\n",
        "        return None\n",
        "\n",
        "    subset_images = extract_subset_to_skiing_169()\n",
        "    if not subset_images:\n",
        "        return None\n",
        "\n",
        "    model, processor = init_ultra_fast_model()\n",
        "\n",
        "    predictions = process_ultra_fast_swig(\n",
        "        model, processor, subset_images,\n",
        "        output_file=\"swig_ultra_fast_small.json\",\n",
        "        batch_size=20,\n",
        "        max_images=50,\n",
        "        save_interval=25\n",
        "    )\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def run_ultra_fast_full_subset():\n",
        "    \"\"\"Ultra-fast full subset processing\"\"\"\n",
        "    img_dir = download_swig_test_dataset()\n",
        "    if not img_dir:\n",
        "        return None\n",
        "\n",
        "    subset_images = extract_subset_to_skiing_169()\n",
        "    if not subset_images:\n",
        "        return None\n",
        "\n",
        "    model, processor = init_ultra_fast_model()\n",
        "\n",
        "    predictions = process_ultra_fast_swig(\n",
        "        model, processor, subset_images,\n",
        "        output_file=\"swig_ultra_fast_full.json\",\n",
        "        batch_size=20,\n",
        "        max_images=None,\n",
        "        save_interval=500\n",
        "    )\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def run_ultra_fast_all_25k():\n",
        "    \"\"\"Ultra-fast processing of all 25,200 images\"\"\"\n",
        "    img_dir = download_swig_test_dataset()\n",
        "    if not img_dir:\n",
        "        return None\n",
        "\n",
        "    # Load all test images\n",
        "    with open(\"test.json\", 'r') as f:\n",
        "        test_data = json.load(f)\n",
        "    all_test_images = list(test_data.keys())\n",
        "\n",
        "    model, processor = init_ultra_fast_model()\n",
        "\n",
        "    predictions = process_ultra_fast_swig(\n",
        "        model, processor, all_test_images,\n",
        "        output_file=\"swig_ultra_fast_all_25k.json\",\n",
        "        batch_size=20,\n",
        "        max_images=None,\n",
        "        save_interval=2000\n",
        "    )\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "lyGaCAqnujpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Execute Processing\n",
        "if __name__ == \"__main__\":\n",
        "    # CHANGE THIS TO SELECT MODE\n",
        "    mode = \"all\"  # Options: \"small\", \"subset\", \"all\"\n",
        "\n",
        "    if mode == \"small\":\n",
        "        results = run_ultra_fast_small_test()\n",
        "    elif mode == \"subset\":\n",
        "        results = run_ultra_fast_full_subset()\n",
        "    elif mode == \"all\":\n",
        "        results = run_ultra_fast_all_25k()\n",
        "\n",
        "    # Generate final output\n",
        "    if results:\n",
        "        final_json = generate_ultra_fast_json_output(results)\n",
        "\n",
        "        # Show sample\n",
        "        if final_json:\n",
        "            print(json.dumps(final_json[0], indent=2))\n",
        "    else:\n",
        "        pass"
      ],
      "metadata": {
        "id": "3Ub-wm0MupOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}