{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMffnR+6Too6PByVr+dMPz8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RArunn/Intent-Identification-Detection/blob/main/swighoi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import torch\n",
        "import gc\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
        "import urllib.request\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "zmzq6JXAKBxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Dataset Download\n",
        "def download_dataset():\n",
        "    img_dir = \"images_512\"\n",
        "\n",
        "    if os.path.exists(img_dir):\n",
        "        return img_dir\n",
        "\n",
        "    if not os.path.exists(\"images_512.zip\"):\n",
        "        result = os.system('aria2c -x 16 -s 16 -o images_512.zip \"https://swig-data-weights.s3.us-east-2.amazonaws.com/images_512.zip\" || wget -O images_512.zip \"https://swig-data-weights.s3.us-east-2.amazonaws.com/images_512.zip\"')\n",
        "        if result != 0:\n",
        "            return None\n",
        "\n",
        "    result = os.system(\"unzip -q images_512.zip\")\n",
        "\n",
        "    if result == 0:\n",
        "        os.remove(\"images_512.zip\")\n",
        "        return img_dir\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "1VE05V_SJ_Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Subset Extraction\n",
        "def extract_image_subset():\n",
        "    cache_file = \"subset_cache.json\"\n",
        "\n",
        "    if os.path.exists(cache_file):\n",
        "        with open(cache_file, 'r') as f:\n",
        "            subset_images = json.load(f)\n",
        "        return subset_images\n",
        "\n",
        "    if not os.path.exists(\"test.json\"):\n",
        "        return None\n",
        "\n",
        "    with open(\"test.json\", 'r') as f:\n",
        "        test_data = json.load(f)\n",
        "\n",
        "    all_test_images = list(test_data.keys())\n",
        "    target_image = \"skiing_169.jpg\"\n",
        "\n",
        "    try:\n",
        "        end_index = all_test_images.index(target_image)\n",
        "        subset_images = all_test_images[:end_index + 1]\n",
        "\n",
        "        with open(cache_file, 'w') as f:\n",
        "            json.dump(subset_images, f)\n",
        "\n",
        "        return subset_images\n",
        "\n",
        "    except ValueError:\n",
        "        return None"
      ],
      "metadata": {
        "id": "3hHWaRWUJ6u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Model Initialization\n",
        "def initialize_model():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "    dummy_image = Image.new('RGB', (224, 224))\n",
        "    dummy_messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": dummy_image}, {\"type\": \"text\", \"text\": \"test\"}]}]\n",
        "    dummy_text = processor.apply_chat_template(dummy_messages, tokenize=False, add_generation_prompt=True)\n",
        "    dummy_inputs = processor(text=[dummy_text], images=[dummy_image], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        _ = model.generate(**dummy_inputs, max_new_tokens=10, do_sample=False)\n",
        "\n",
        "    del dummy_inputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "kFRN3r2BJ27k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Prompt Definition\n",
        "hoi_detection_prompt = \"\"\"Analyze this image for human-object interactions. For each interaction you find, provide:\n",
        "\n",
        "1. What the person is doing\n",
        "2. What object they're interacting with\n",
        "3. Location of the person as [x, y, width, height]\n",
        "4. Location of the object as [x, y, width, height]\n",
        "\n",
        "Format your response as:\n",
        "Person at [x,y,w,h] doing ACTION with OBJECT at [x,y,w,h]\n",
        "\n",
        "Only include clear, visible interactions. Maximum 10 interactions per image.\"\"\""
      ],
      "metadata": {
        "id": "KKjG6yxAJzfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Dataset Class\n",
        "class SWiGDataset(Dataset):\n",
        "    def __init__(self, test_image_files, max_images=None):\n",
        "        self.img_dir = \"images_512\"\n",
        "        self.test_image_files = test_image_files\n",
        "\n",
        "        self.valid_image_files = []\n",
        "        for img_file in test_image_files:\n",
        "            image_path = os.path.join(self.img_dir, img_file)\n",
        "            if os.path.exists(image_path):\n",
        "                self.valid_image_files.append(img_file)\n",
        "\n",
        "        if max_images:\n",
        "            self.valid_image_files = self.valid_image_files[:max_images]\n",
        "\n",
        "        self.image_metadata = {}\n",
        "        for img_file in self.valid_image_files:\n",
        "            image_path = os.path.join(self.img_dir, img_file)\n",
        "            try:\n",
        "                with Image.open(image_path) as img:\n",
        "                    self.image_metadata[img_file] = img.size\n",
        "            except:\n",
        "                self.image_metadata[img_file] = (512, 512)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file = self.valid_image_files[idx]\n",
        "        image_id = img_file.replace('.jpg', '')\n",
        "        image_path = os.path.join(self.img_dir, img_file)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            original_size = self.image_metadata.get(img_file, image.size)\n",
        "\n",
        "            max_size = 448\n",
        "            if max(original_size) > max_size:\n",
        "                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
        "\n",
        "            return {\n",
        "                'image': image,\n",
        "                'image_id': image_id,\n",
        "                'original_size': original_size,\n",
        "                'resized_size': image.size,\n",
        "                'filename': img_file\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'image': Image.new('RGB', (224, 224)),\n",
        "                'image_id': image_id,\n",
        "                'original_size': (224, 224),\n",
        "                'resized_size': (224, 224),\n",
        "                'filename': img_file\n",
        "            }"
      ],
      "metadata": {
        "id": "4jRaWS54Jw6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: HOI Prediction Extraction\n",
        "def extract_hoi_predictions(text, image_data, max_detections=10):\n",
        "    results = []\n",
        "\n",
        "    pattern_with_conf = re.compile(r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+confidence\\s+([0-9]*\\.?[0-9]+)', re.IGNORECASE)\n",
        "    pattern_no_conf = re.compile(r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]', re.IGNORECASE)\n",
        "\n",
        "    invalid_objects = {'object', 'objects', 'thing', 'item', 'stuff', 'it'}\n",
        "\n",
        "    matches = pattern_with_conf.findall(text)\n",
        "    if not matches:\n",
        "        matches = [(m + (0.7,)) for m in pattern_no_conf.findall(text)]\n",
        "\n",
        "    scale_x = image_data['original_size'][0] / image_data['resized_size'][0]\n",
        "    scale_y = image_data['original_size'][1] / image_data['resized_size'][1]\n",
        "    width, height = image_data['original_size']\n",
        "\n",
        "    for match in matches[:max_detections]:\n",
        "        try:\n",
        "            if len(match) == 11:\n",
        "                px, py, pw, ph, verb, obj, ox, oy, ow, oh, score = match\n",
        "                score = float(score)\n",
        "            else:\n",
        "                px, py, pw, ph, verb, obj, ox, oy, ow, oh = match\n",
        "                score = 0.7\n",
        "\n",
        "            px, py, pw, ph = int(px), int(py), int(pw), int(ph)\n",
        "            ox, oy, ow, oh = int(ox), int(oy), int(ow), int(oh)\n",
        "\n",
        "            if (len(obj) < 2 or obj.lower() in invalid_objects or\n",
        "                px < 0 or py < 0 or pw <= 0 or ph <= 0 or\n",
        "                ox < 0 or oy < 0 or ow <= 0 or oh <= 0):\n",
        "                continue\n",
        "\n",
        "            subject_box = [\n",
        "                max(0, min(width, int(px * scale_x))),\n",
        "                max(0, min(height, int(py * scale_y))),\n",
        "                max(0, min(width, int((px + pw) * scale_x))),\n",
        "                max(0, min(height, int((py + ph) * scale_y)))\n",
        "            ]\n",
        "\n",
        "            object_box = [\n",
        "                max(0, min(width, int(ox * scale_x))),\n",
        "                max(0, min(height, int(oy * scale_y))),\n",
        "                max(0, min(width, int((ox + ow) * scale_x))),\n",
        "                max(0, min(height, int((oy + oh) * scale_y)))\n",
        "            ]\n",
        "\n",
        "            if (subject_box[2] <= subject_box[0] or subject_box[3] <= subject_box[1] or\n",
        "                object_box[2] <= object_box[0] or object_box[3] <= object_box[1]):\n",
        "                continue\n",
        "\n",
        "            results.append({\n",
        "                \"subject_box\": subject_box,\n",
        "                \"object_box\": object_box,\n",
        "                \"subject_category\": \"person\",\n",
        "                \"object_category\": obj.lower(),\n",
        "                \"verb\": verb.lower(),\n",
        "                \"score\": round(max(0.1, min(1.0, score)), 4)\n",
        "            })\n",
        "\n",
        "        except (ValueError, IndexError):\n",
        "            continue\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "AmS3DTySJsZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Batch Inference\n",
        "def batch_inference(batch_data, model, processor):\n",
        "    valid_batch = [item for item in batch_data if item['image_id'] != 'invalid']\n",
        "    if not valid_batch:\n",
        "        return []\n",
        "\n",
        "    images = [item['image'] for item in valid_batch]\n",
        "\n",
        "    messages_list = []\n",
        "    for image in images:\n",
        "        messages_list.append([{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": hoi_detection_prompt}\n",
        "            ]\n",
        "        }])\n",
        "\n",
        "    try:\n",
        "        processor.tokenizer.padding_side = 'left'\n",
        "\n",
        "        texts = [processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "                 for msgs in messages_list]\n",
        "\n",
        "        inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            max_length=2048,\n",
        "            truncation=True\n",
        "        ).to(model.device, non_blocking=True)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                output_ids = model.generate(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    pixel_values=inputs.get('pixel_values', None),\n",
        "                    image_grid_thw=inputs.get('image_grid_thw', None),\n",
        "                    max_new_tokens=150,\n",
        "                    do_sample=False,\n",
        "                    use_cache=True,\n",
        "                    num_beams=1,\n",
        "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "                    eos_token_id=processor.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "        generated_ids = output_ids[:, inputs['input_ids'].shape[1]:]\n",
        "        responses = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        batch_results = []\n",
        "        for data, response in zip(valid_batch, responses):\n",
        "            hoi_predictions = extract_hoi_predictions(response, data, max_detections=10)\n",
        "            batch_results.append({\n",
        "                'image_id': data['image_id'],\n",
        "                'hoi_prediction': hoi_predictions\n",
        "            })\n",
        "\n",
        "        del inputs, output_ids, generated_ids\n",
        "        torch.cuda.empty_cache()\n",
        "        return batch_results\n",
        "\n",
        "    except Exception as e:\n",
        "        return []"
      ],
      "metadata": {
        "id": "WGSb1cB3Joas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Processing Pipeline\n",
        "def process_images(model, processor, test_image_files, output_file=\"results.json\",\n",
        "                   batch_size=20, max_images=None, save_interval=1000):\n",
        "    dataset = SWiGDataset(test_image_files, max_images)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=12,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda x: x,\n",
        "        persistent_workers=True,\n",
        "        prefetch_factor=6,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    all_predictions = []\n",
        "    processed_count = 0\n",
        "    total_hoi_count = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Processing\", unit=\"batch\")\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(pbar):\n",
        "        try:\n",
        "            batch_results = batch_inference(batch_data, model, processor)\n",
        "            all_predictions.extend(batch_results)\n",
        "            processed_count += len(batch_data)\n",
        "\n",
        "            batch_hoi_count = sum(len(pred['hoi_prediction']) for pred in batch_results)\n",
        "            total_hoi_count += batch_hoi_count\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Images': processed_count,\n",
        "                'HOI': total_hoi_count,\n",
        "                'Avg_HOI': f\"{total_hoi_count/processed_count:.1f}\"\n",
        "            })\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            if processed_count % save_interval == 0:\n",
        "                checkpoint_file = f\"{output_file}.checkpoint_{processed_count}.json\"\n",
        "                with open(checkpoint_file, 'w') as f:\n",
        "                    json.dump(all_predictions, f, indent=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(all_predictions, f, indent=2)\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "BObWSEz9Jj3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: JSON Output Generation\n",
        "def generate_json_output(predictions, output_file=\"swighoi.json\"):\n",
        "    if not predictions:\n",
        "        return\n",
        "\n",
        "    final_output = []\n",
        "\n",
        "    for prediction in predictions:\n",
        "        image_id_str = prediction['image_id']\n",
        "\n",
        "        if image_id_str.endswith('.jpg'):\n",
        "            image_id = image_id_str[:-4]\n",
        "        else:\n",
        "            image_id = image_id_str\n",
        "\n",
        "        formatted_prediction = {\n",
        "            \"image_id\": image_id,\n",
        "            \"hoi_prediction\": prediction.get('hoi_prediction', [])[:10]\n",
        "        }\n",
        "\n",
        "        final_output.append(formatted_prediction)\n",
        "\n",
        "    def format_json(data):\n",
        "        json_str = json.dumps(data, indent=2, separators=(',', ': '))\n",
        "        array_pattern = re.compile(r'\\[\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\]')\n",
        "        json_str = array_pattern.sub(r'[\\1, \\2, \\3, \\4]', json_str)\n",
        "        return json_str\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(format_json(final_output))\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "8uIWwKZpJf8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Execution Functions\n",
        "def run_test_subset():\n",
        "    img_dir = download_dataset()\n",
        "    if not img_dir:\n",
        "        return None\n",
        "\n",
        "    subset_images = extract_image_subset()\n",
        "    if not subset_images:\n",
        "        return None\n",
        "\n",
        "    model, processor = initialize_model()\n",
        "\n",
        "    predictions = process_images(\n",
        "        model, processor, subset_images,\n",
        "        output_file=\"test_results.json\",\n",
        "        batch_size=20,\n",
        "        max_images=50,\n",
        "        save_interval=25\n",
        "    )\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def run_full_subset():\n",
        "    img_dir = download_dataset()\n",
        "    if not img_dir:\n",
        "        return None\n",
        "\n",
        "    subset_images = extract_image_subset()\n",
        "    if not subset_images:\n",
        "        return None\n",
        "\n",
        "    model, processor = initialize_model()\n",
        "\n",
        "    predictions = process_images(\n",
        "        model, processor, subset_images,\n",
        "        output_file=\"subset_results.json\",\n",
        "        batch_size=20,\n",
        "        max_images=None,\n",
        "        save_interval=500\n",
        "    )\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def run_full_dataset():\n",
        "    img_dir = download_dataset()\n",
        "    if not img_dir:\n",
        "        return None\n",
        "\n",
        "    with open(\"test.json\", 'r') as f:\n",
        "        test_data = json.load(f)\n",
        "    all_test_images = list(test_data.keys())\n",
        "\n",
        "    model, processor = initialize_model()\n",
        "\n",
        "    predictions = process_images(\n",
        "        model, processor, all_test_images,\n",
        "        output_file=\"full_results.json\",\n",
        "        batch_size=20,\n",
        "        max_images=None,\n",
        "        save_interval=2000\n",
        "    )\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "B7X08mq-JcVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    mode = \"all\"\n",
        "\n",
        "    if mode == \"small\":\n",
        "        results = run_test_subset()\n",
        "    elif mode == \"subset\":\n",
        "        results = run_full_subset()\n",
        "    elif mode == \"all\":\n",
        "        results = run_full_dataset()\n",
        "\n",
        "    if results:\n",
        "        final_json = generate_json_output(results)"
      ],
      "metadata": {
        "id": "abncmw4KJXYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}