{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RArunn/Intent-Identification-Detection/blob/main/vcoco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import gc\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# GPU Memory Optimization Configuration\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "t_0DIY59DRsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_coco_validation_dataset():\n",
        "    \"\"\"Download and extract COCO validation dataset\"\"\"\n",
        "    val_dir = \"val2014\"\n",
        "    if os.path.exists(val_dir):\n",
        "        print(f\"Dataset directory '{val_dir}' already exists.\")\n",
        "        return val_dir\n",
        "\n",
        "    print(\"Downloading COCO validation dataset...\")\n",
        "    os.system(\"wget -O val2014.zip http://images.cocodataset.org/zips/val2014.zip\")\n",
        "\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(\"val2014.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    os.remove(\"val2014.zip\")\n",
        "    print(f\"Dataset ready at: {val_dir}\")\n",
        "    return val_dir"
      ],
      "metadata": {
        "id": "twRuib_xDS_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "val_dir = download_coco_validation_dataset()"
      ],
      "metadata": {
        "id": "Jvny9rwKDXKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_vision_language_model():\n",
        "    \"\"\"Initialize model optimized for speed\"\"\"\n",
        "    print(\"Initializing vision-language model...\")\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Compile for speed\n",
        "    print(\"Compiling model for optimization...\")\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "    print(\"Model initialization complete!\")\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "qWvzeVIlDYxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interaction Detection Prompt Template\n",
        "interaction_detection_prompt = \"\"\"Analyze this image for human-object interactions. For each interaction you find, provide:\n",
        "\n",
        "1. What the person is doing\n",
        "2. What object they're interacting with\n",
        "3. Location of the person as [x, y, width, height]\n",
        "4. Location of the object as [x, y, width, height]\n",
        "\n",
        "Format your response as:\n",
        "Person at [x,y,w,h] doing ACTION with OBJECT at [x,y,w,h]\n",
        "\n",
        "Only include clear, visible interactions. Maximum 10 interactions per image.\"\"\"\n",
        "\n",
        "print(\"Interaction detection prompt defined.\")"
      ],
      "metadata": {
        "id": "2eCTuA7kDdQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class COCOImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, max_images=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "\n",
        "        if max_images:\n",
        "            self.image_files = self.image_files[:max_images]\n",
        "            print(f\"Limited dataset to {max_images} images\")\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.image_files)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file = self.image_files[idx]\n",
        "        image_id = int(img_file.replace('COCO_val2014_', '').replace('.jpg', ''))\n",
        "        image_path = os.path.join(self.image_dir, img_file)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            original_size = image.size\n",
        "\n",
        "            # Smaller size for speed\n",
        "            max_size = 448\n",
        "            if max(original_size) > max_size:\n",
        "                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
        "\n",
        "            return {\n",
        "                'image': image,\n",
        "                'image_id': image_id,\n",
        "                'original_size': original_size,\n",
        "                'resized_size': image.size,\n",
        "                'filename': img_file\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_file}: {e}\")\n",
        "            return {\n",
        "                'image': Image.new('RGB', (224, 224)),\n",
        "                'image_id': -1,\n",
        "                'original_size': (224, 224),\n",
        "                'resized_size': (224, 224),\n",
        "                'filename': img_file\n",
        "            }"
      ],
      "metadata": {
        "id": "JyjY1lykDlPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_interaction_coordinates(text, image_data):\n",
        "    \"\"\"Extract and validate interaction coordinates from model response\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Primary pattern: \"Person at [x,y,w,h] doing ACTION with OBJECT at [x,y,w,h] confidence 0.85\"\n",
        "    pattern_with_conf = r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+confidence\\s+([0-9]*\\.?[0-9]+)'\n",
        "\n",
        "    # Fallback pattern: without confidence (fallback to 0.7)\n",
        "    pattern_no_conf = r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]'\n",
        "\n",
        "    # Try primary pattern first\n",
        "    matches = list(re.finditer(pattern_with_conf, text.lower()))\n",
        "\n",
        "    # If no matches with confidence, try fallback pattern\n",
        "    if not matches:\n",
        "        matches = [(match, 0.7) for match in re.finditer(pattern_no_conf, text.lower())]\n",
        "    else:\n",
        "        matches = [(match, None) for match in matches]\n",
        "\n",
        "    for match_data in matches:\n",
        "        match, fallback_conf = match_data\n",
        "        try:\n",
        "            # Extract coordinates\n",
        "            px, py, pw, ph = map(int, match.groups()[:4])\n",
        "            action = match.group(5).strip()\n",
        "            obj = match.group(6).strip()\n",
        "\n",
        "            # Object validation for interaction quality\n",
        "            if len(obj) < 2 or obj.lower() in ['object', 'objects', 'thing', 'item', 'stuff', 'it']:\n",
        "                continue\n",
        "            ox, oy, ow, oh = map(int, match.groups()[6:10])\n",
        "\n",
        "            # Get confidence\n",
        "            if fallback_conf is not None:\n",
        "                confidence = fallback_conf\n",
        "            else:\n",
        "                confidence = float(match.group(11))\n",
        "                confidence = max(0.1, min(1.0, confidence))\n",
        "\n",
        "            # Validate coordinates before processing\n",
        "            if px < 0 or py < 0 or pw <= 0 or ph <= 0 or ox < 0 or oy < 0 or ow <= 0 or oh <= 0:\n",
        "                continue\n",
        "\n",
        "            # Scale coordinates to original image size\n",
        "            scale_x = image_data['original_size'][0] / image_data['resized_size'][0]\n",
        "            scale_y = image_data['original_size'][1] / image_data['resized_size'][1]\n",
        "\n",
        "            # Convert [x,y,w,h] to [x1,y1,x2,y2] and scale\n",
        "            person_box = [\n",
        "                int(px * scale_x),\n",
        "                int(py * scale_y),\n",
        "                int((px + pw) * scale_x),\n",
        "                int((py + ph) * scale_y)\n",
        "            ]\n",
        "\n",
        "            object_box = [\n",
        "                int(ox * scale_x),\n",
        "                int(oy * scale_y),\n",
        "                int((ox + ow) * scale_x),\n",
        "                int((oy + oh) * scale_y)\n",
        "            ]\n",
        "\n",
        "            # Validate boxes are within image bounds\n",
        "            width, height = image_data['original_size']\n",
        "            person_box = [\n",
        "                max(0, min(width, person_box[0])),\n",
        "                max(0, min(height, person_box[1])),\n",
        "                max(0, min(width, person_box[2])),\n",
        "                max(0, min(height, person_box[3]))\n",
        "            ]\n",
        "\n",
        "            object_box = [\n",
        "                max(0, min(width, object_box[0])),\n",
        "                max(0, min(height, object_box[1])),\n",
        "                max(0, min(width, object_box[2])),\n",
        "                max(0, min(height, object_box[3]))\n",
        "            ]\n",
        "\n",
        "            # Skip invalid boxes\n",
        "            if (person_box[2] <= person_box[0] or person_box[3] <= person_box[1] or\n",
        "                object_box[2] <= object_box[0] or object_box[3] <= object_box[1]):\n",
        "                continue\n",
        "\n",
        "            result = {\n",
        "                'image_id': image_data['image_id'],\n",
        "                'person_box': person_box,\n",
        "                f'{action}_agent': confidence,\n",
        "                f'{action}_{obj}': object_box + [confidence]\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "        except (ValueError, IndexError, AttributeError):\n",
        "            continue\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "PN7jpv3-DoTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_batch(batch_data, model, processor):\n",
        "    \"\"\"Process batch of images for interaction detection\"\"\"\n",
        "    valid_batch = [item for item in batch_data if item['image_id'] != -1]\n",
        "    if not valid_batch:\n",
        "        return []\n",
        "\n",
        "    images = [item['image'] for item in valid_batch]\n",
        "\n",
        "    # Create messages\n",
        "    messages_list = []\n",
        "    for image in images:\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": interaction_detection_prompt}\n",
        "            ]\n",
        "        }]\n",
        "        messages_list.append(messages)\n",
        "\n",
        "    try:\n",
        "        texts = [processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "                 for msgs in messages_list]\n",
        "\n",
        "        inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            max_length=1024\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Fast generation\n",
        "        with torch.inference_mode():\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                output_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=150,  # Increased for more detailed responses\n",
        "                    do_sample=False,     # Greedy for speed\n",
        "                    use_cache=True,\n",
        "                    pad_token_id=processor.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "        generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "        responses = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Extract interactions from all responses\n",
        "        all_results = []\n",
        "        for data, response in zip(valid_batch, responses):\n",
        "            results = extract_interaction_coordinates(response, data)\n",
        "            all_results.extend(results)\n",
        "\n",
        "        # Cleanup\n",
        "        del inputs, output_ids, generated_ids\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "lOtUSsU3DrTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_formatted_json(data, filename):\n",
        "    \"\"\"Save JSON with arrays formatted on single lines\"\"\"\n",
        "    import json\n",
        "    import re\n",
        "\n",
        "    # Convert to JSON with standard formatting\n",
        "    json_str = json.dumps(data, indent=2)\n",
        "\n",
        "    # Pattern to match numerical arrays\n",
        "    array_pattern = r'\\[\\s*(\\d+(?:\\.\\d+)?(?:\\s*,\\s*\\d+(?:\\.\\d+)?)*)\\s*\\]'\n",
        "\n",
        "    def format_array_inline(match):\n",
        "        # Format numbers on single line with consistent spacing\n",
        "        numbers = match.group(1)\n",
        "        # Standardize spacing between numbers\n",
        "        numbers = re.sub(r'\\s*,\\s*', ', ', numbers)\n",
        "        return f'[{numbers}]'\n",
        "\n",
        "    # Apply formatting optimization\n",
        "    json_str = re.sub(array_pattern, format_array_inline, json_str)\n",
        "\n",
        "    # Write to file\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(json_str)\n",
        "\n",
        "    print(f\"Results saved to: {filename}\")"
      ],
      "metadata": {
        "id": "N-6FePwdDyji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_coco_dataset(model, processor, output_file=\"coco_hoi_full_fast.json\",\n",
        "                      batch_size=16, max_images=None, save_interval=1000):\n",
        "    \"\"\"Process COCO dataset for human-object interaction detection\"\"\"\n",
        "\n",
        "    dataset = COCOImageDataset('val2014', max_images)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=8,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda x: x,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    # Group detections by image identifier\n",
        "    detections_by_image = {}  # {image_id: [detection_results]}\n",
        "    processed_images = []\n",
        "    processed_count = 0\n",
        "\n",
        "    print(f\"Starting processing with batch_size={batch_size}, max_images={max_images}\")\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=\"Processing images\")):\n",
        "        try:\n",
        "            # Process current batch\n",
        "            batch_results = process_image_batch(batch_data, model, processor)\n",
        "\n",
        "            # Group results by image_id\n",
        "            for result in batch_results:\n",
        "                image_id = result['image_id']\n",
        "                if image_id not in detections_by_image:\n",
        "                    detections_by_image[image_id] = []\n",
        "                detections_by_image[image_id].append(result)\n",
        "\n",
        "            # Track processed images\n",
        "            for data in batch_data:\n",
        "                if data['image_id'] != -1:\n",
        "                    processed_images.append({\n",
        "                        'image_id': data['image_id'],\n",
        "                        'filename': data['filename']\n",
        "                    })\n",
        "\n",
        "            processed_count += len(batch_data)\n",
        "\n",
        "            # Clean up\n",
        "            for data in batch_data:\n",
        "                if hasattr(data['image'], 'close'):\n",
        "                    data['image'].close()\n",
        "\n",
        "            # Save progress checkpoint\n",
        "            if processed_count % save_interval == 0:\n",
        "                # Generate checkpoint data\n",
        "                checkpoint_results = []\n",
        "                for img_id in sorted(detections_by_image.keys()):\n",
        "                    checkpoint_results.extend(detections_by_image[img_id])\n",
        "\n",
        "                checkpoint_data = {\n",
        "                    'results': checkpoint_results,\n",
        "                    'processed_count': processed_count,\n",
        "                    'detection_count': len(checkpoint_results)\n",
        "                }\n",
        "\n",
        "                checkpoint_file = f\"{output_file}.checkpoint_{processed_count}.json\"\n",
        "                save_formatted_json(checkpoint_data, checkpoint_file)\n",
        "                print(f\"Checkpoint saved: {processed_count} images processed\")\n",
        "\n",
        "            # Periodic memory management\n",
        "            if batch_idx % 50 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Generate final output dataset\n",
        "    final_results = []\n",
        "\n",
        "    # Process all images in sequential order\n",
        "    all_processed_images = sorted(processed_images, key=lambda x: x['image_id'])\n",
        "\n",
        "    for img_info in all_processed_images:\n",
        "        img_id = img_info['image_id']\n",
        "\n",
        "        if img_id in detections_by_image:\n",
        "            # Include all detected interactions for this image\n",
        "            final_results.extend(detections_by_image[img_id])\n",
        "        else:\n",
        "            # Record image with no detected interactions\n",
        "            final_results.append({\n",
        "                'image_id': img_id,\n",
        "                'detections': []  # Empty array indicates no interactions found\n",
        "            })\n",
        "\n",
        "    final_output = {\n",
        "        'results': final_results,  # All images ordered by image_id\n",
        "        'metadata': {\n",
        "            'total_images_processed': processed_count,\n",
        "            'total_detections': sum(1 for r in final_results if 'person_box' in r),\n",
        "            'images_with_detections': len(detections_by_image),\n",
        "            'images_without_detections': processed_count - len(detections_by_image),\n",
        "            'detection_rate': f\"{len(detections_by_image)/processed_count*100:.1f}%\" if processed_count > 0 else \"0%\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save final results\n",
        "    save_formatted_json(final_output, output_file)\n",
        "\n",
        "    print(f\"\\nProcessing complete!\")\n",
        "    print(f\"Total images processed: {processed_count}\")\n",
        "    print(f\"Total detections: {final_output['metadata']['total_detections']}\")\n",
        "    print(f\"Detection rate: {final_output['metadata']['detection_rate']}\")\n",
        "\n",
        "    return final_results"
      ],
      "metadata": {
        "id": "5Ip4VL8CD06z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation_test():\n",
        "    \"\"\"Run validation test on small image subset\"\"\"\n",
        "    print(\"Running validation test...\")\n",
        "    model, processor = initialize_vision_language_model()\n",
        "\n",
        "    results = process_coco_dataset(\n",
        "        model,\n",
        "        processor,\n",
        "        output_file=\"validation_test_results.json\",\n",
        "        batch_size=16,\n",
        "        max_images=50,\n",
        "        save_interval=25\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        print(f\"\\nSample results (first 3):\")\n",
        "        for i, result in enumerate(results[:3]):\n",
        "            print(f\"Result {i+1}:\", json.dumps(result, indent=2))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Ob8nTJrRD9SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_full_dataset_processing():\n",
        "    \"\"\"Process complete COCO validation dataset for interaction detection\"\"\"\n",
        "    print(\"Starting full dataset processing...\")\n",
        "    model, processor = initialize_vision_language_model()\n",
        "\n",
        "    # Optimized batch size for high-performance GPU\n",
        "    batch_size = 28\n",
        "\n",
        "    results = process_coco_dataset(\n",
        "        model,\n",
        "        processor,\n",
        "        output_file=\"coco_hoi_full_fast.json\",\n",
        "        batch_size=batch_size,\n",
        "        max_images=None,  # Process complete dataset\n",
        "        save_interval=2000\n",
        "    )\n",
        "\n",
        "    # Statistical analysis of detected interactions\n",
        "    if results:\n",
        "        print(\"\\nAnalyzing interaction statistics...\")\n",
        "        actions = {}\n",
        "        objects = {}\n",
        "        for result in results:\n",
        "            for key in result.keys():\n",
        "                if key.endswith('_agent'):\n",
        "                    action = key.replace('_agent', '')\n",
        "                    actions[action] = actions.get(action, 0) + 1\n",
        "                elif '_' in key and key not in ['image_id', 'person_box']:\n",
        "                    parts = key.split('_', 1)\n",
        "                    if len(parts) == 2:\n",
        "                        obj = parts[1]\n",
        "                        objects[obj] = objects.get(obj, 0) + 1\n",
        "\n",
        "        print(f\"Top 10 detected actions: {dict(sorted(actions.items(), key=lambda x: x[1], reverse=True)[:10])}\")\n",
        "        print(f\"Top 10 detected objects: {dict(sorted(objects.items(), key=lambda x: x[1], reverse=True)[:10])}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "QFSxQtrREBFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run validation test\n",
        "# validation_results = run_validation_test()\n",
        "\n",
        "# Run full dataset processing\n",
        "final_results = run_full_dataset_processing()"
      ],
      "metadata": {
        "id": "1--sKIlNEDwV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOTuB3G9v9YGhQK6Q/r5JW5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}