{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RArunn/Intent-Identification-Detection/blob/main/vcoco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Fast HOI Detection Code for Colab Pro A100\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import gc\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# A100 optimization\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "R1ySuTlmr2rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Dataset Download\n",
        "def download_coco_val2014():\n",
        "    val_dir = \"val2014\"\n",
        "    if os.path.exists(val_dir):\n",
        "        return val_dir\n",
        "\n",
        "    os.system(\"wget -O val2014.zip http://images.cocodataset.org/zips/val2014.zip\")\n",
        "\n",
        "    with zipfile.ZipFile(\"val2014.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    os.remove(\"val2014.zip\")\n",
        "    return val_dir\n",
        "\n",
        "# Download dataset\n",
        "val_dir = download_coco_val2014()"
      ],
      "metadata": {
        "id": "pUbBN1Hkr5jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Fast Model Initialization\n",
        "def init_model_fast():\n",
        "    \"\"\"Initialize model optimized for speed\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Compile for speed\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "AVp48OYVr8Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Prompt\n",
        "fast_prompt = \"\"\"Analyze this image for human-object interactions. For each interaction you find, provide:\n",
        "\n",
        "1. What the person is doing\n",
        "2. What object they're interacting with\n",
        "3. Location of the person as [x, y, width, height]\n",
        "4. Location of the object as [x, y, width, height]\n",
        "\n",
        "Format your response as:\n",
        "Person at [x,y,w,h] doing ACTION with OBJECT at [x,y,w,h]\n",
        "\n",
        "Only include clear, visible interactions. Maximum 10 interactions per image.\"\"\""
      ],
      "metadata": {
        "id": "fomjMtIOr-7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4:\n",
        "class FastCOCODataset(Dataset):\n",
        "    def __init__(self, image_dir, max_images=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "\n",
        "        if max_images:\n",
        "            self.image_files = self.image_files[:max_images]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_file = self.image_files[idx]\n",
        "        image_id = int(img_file.replace('COCO_val2014_', '').replace('.jpg', ''))\n",
        "        image_path = os.path.join(self.image_dir, img_file)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            original_size = image.size\n",
        "\n",
        "            # Smaller size for speed\n",
        "            max_size = 448\n",
        "            if max(original_size) > max_size:\n",
        "                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
        "\n",
        "            return {\n",
        "                'image': image,\n",
        "                'image_id': image_id,\n",
        "                'original_size': original_size,\n",
        "                'resized_size': image.size,\n",
        "                'filename': img_file\n",
        "            }\n",
        "        except:\n",
        "            return {\n",
        "                'image': Image.new('RGB', (224, 224)),\n",
        "                'image_id': -1,\n",
        "                'original_size': (224, 224),\n",
        "                'resized_size': (224, 224),\n",
        "                'filename': img_file\n",
        "            }"
      ],
      "metadata": {
        "id": "29w0mO2psB9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Extract Real Coordinates from Qwen Response (ROBUST + DYNAMIC CONFIDENCE)\n",
        "def extract_interactions_fast(text, image_data):\n",
        "    \"\"\"Extract real coordinates from Qwen's response - ROBUST with fallback confidence\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Primary pattern: \"Person at [x,y,w,h] doing ACTION with OBJECT at [x,y,w,h] confidence 0.85\"\n",
        "    pattern_with_conf = r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+confidence\\s+([0-9]*\\.?[0-9]+)'\n",
        "\n",
        "    # Fallback pattern: without confidence (fallback to 0.7)\n",
        "    pattern_no_conf = r'person\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\s+(?:doing\\s+)?(\\w+)\\s+(?:with\\s+)?(\\w+)\\s+at\\s+\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]'\n",
        "\n",
        "    # Try primary pattern first\n",
        "    matches = list(re.finditer(pattern_with_conf, text.lower()))\n",
        "\n",
        "    # If no matches with confidence, try fallback pattern\n",
        "    if not matches:\n",
        "        matches = [(match, 0.7) for match in re.finditer(pattern_no_conf, text.lower())]\n",
        "    else:\n",
        "        matches = [(match, None) for match in matches]\n",
        "\n",
        "    for match_data in matches:\n",
        "        match, fallback_conf = match_data\n",
        "        try:\n",
        "            # Extract coordinates\n",
        "            px, py, pw, ph = map(int, match.groups()[:4])\n",
        "            action = match.group(5).strip()\n",
        "            obj = match.group(6).strip()\n",
        "\n",
        "            # Smart object validation - keep quality, don't lose real interactions\n",
        "            if len(obj) < 2 or obj.lower() in ['object', 'objects', 'thing', 'item', 'stuff', 'it']:\n",
        "                continue\n",
        "            ox, oy, ow, oh = map(int, match.groups()[6:10])\n",
        "\n",
        "            # Get confidence\n",
        "            if fallback_conf is not None:\n",
        "                confidence = fallback_conf\n",
        "            else:\n",
        "                confidence = float(match.group(11))\n",
        "                confidence = max(0.1, min(1.0, confidence))\n",
        "\n",
        "            # Validate coordinates before processing\n",
        "            if px < 0 or py < 0 or pw <= 0 or ph <= 0 or ox < 0 or oy < 0 or ow <= 0 or oh <= 0:\n",
        "                continue\n",
        "\n",
        "            # Scale coordinates to original image size\n",
        "            scale_x = image_data['original_size'][0] / image_data['resized_size'][0]\n",
        "            scale_y = image_data['original_size'][1] / image_data['resized_size'][1]\n",
        "\n",
        "            # Convert [x,y,w,h] to [x1,y1,x2,y2] and scale\n",
        "            person_box = [\n",
        "                int(px * scale_x),\n",
        "                int(py * scale_y),\n",
        "                int((px + pw) * scale_x),\n",
        "                int((py + ph) * scale_y)\n",
        "            ]\n",
        "\n",
        "            object_box = [\n",
        "                int(ox * scale_x),\n",
        "                int(oy * scale_y),\n",
        "                int((ox + ow) * scale_x),\n",
        "                int((oy + oh) * scale_y)\n",
        "            ]\n",
        "\n",
        "            # Validate boxes are within image bounds\n",
        "            width, height = image_data['original_size']\n",
        "            person_box = [\n",
        "                max(0, min(width, person_box[0])),\n",
        "                max(0, min(height, person_box[1])),\n",
        "                max(0, min(width, person_box[2])),\n",
        "                max(0, min(height, person_box[3]))\n",
        "            ]\n",
        "\n",
        "            object_box = [\n",
        "                max(0, min(width, object_box[0])),\n",
        "                max(0, min(height, object_box[1])),\n",
        "                max(0, min(width, object_box[2])),\n",
        "                max(0, min(height, object_box[3]))\n",
        "            ]\n",
        "\n",
        "            # Skip invalid boxes\n",
        "            if (person_box[2] <= person_box[0] or person_box[3] <= person_box[1] or\n",
        "                object_box[2] <= object_box[0] or object_box[3] <= object_box[1]):\n",
        "                continue\n",
        "\n",
        "            result = {\n",
        "                'image_id': image_data['image_id'],\n",
        "                'person_box': person_box,\n",
        "                f'{action}_agent': confidence,\n",
        "                f'{action}_{obj}': object_box + [confidence]\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "        except (ValueError, IndexError, AttributeError):\n",
        "            continue\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "wfDMR8m9sHd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Fast Batch Inference\n",
        "def fast_batch_inference(batch_data, model, processor):\n",
        "    \"\"\"Optimized batch inference for speed\"\"\"\n",
        "    valid_batch = [item for item in batch_data if item['image_id'] != -1]\n",
        "    if not valid_batch:\n",
        "        return []\n",
        "\n",
        "    images = [item['image'] for item in valid_batch]\n",
        "\n",
        "    # Create messages\n",
        "    messages_list = []\n",
        "    for image in images:\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": fast_prompt}\n",
        "            ]\n",
        "        }]\n",
        "        messages_list.append(messages)\n",
        "\n",
        "    try:\n",
        "        texts = [processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "                 for msgs in messages_list]\n",
        "\n",
        "        inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            max_length=1024\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Fast generation\n",
        "        with torch.inference_mode():\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                output_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=150,  # Increased for more detailed responses\n",
        "                    do_sample=False,     # Greedy for speed\n",
        "                    use_cache=True,\n",
        "                    pad_token_id=processor.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "        generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "        responses = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Extract interactions from all responses\n",
        "        all_results = []\n",
        "        for data, response in zip(valid_batch, responses):\n",
        "            results = extract_interactions_fast(response, data)\n",
        "            all_results.extend(results)\n",
        "\n",
        "        # Cleanup\n",
        "        del inputs, output_ids, generated_ids\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        return []"
      ],
      "metadata": {
        "id": "uV0Fy18xsLCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Fast Processing Function (Fixed to group by image)\n",
        "def process_coco_fast(model, processor, output_file=\"coco_hoi_fast.json\",\n",
        "                      batch_size=16, max_images=None, save_interval=1000):\n",
        "    \"\"\"Fast processing optimized for A100 - grouped by image\"\"\"\n",
        "\n",
        "    dataset = FastCOCODataset('val2014', max_images)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=8,      # More workers for speed\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda x: x,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    # Group detections by image\n",
        "    detections_by_image = {}  # {image_id: [detections]}\n",
        "    processed_images = []\n",
        "    processed_count = 0\n",
        "\n",
        "    for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=\"Fast processing\")):\n",
        "        try:\n",
        "            # Fast batch inference\n",
        "            batch_results = fast_batch_inference(batch_data, model, processor)\n",
        "\n",
        "            # Group results by image_id\n",
        "            for result in batch_results:\n",
        "                image_id = result['image_id']\n",
        "                if image_id not in detections_by_image:\n",
        "                    detections_by_image[image_id] = []\n",
        "                detections_by_image[image_id].append(result)\n",
        "\n",
        "            # Track processed images\n",
        "            for data in batch_data:\n",
        "                if data['image_id'] != -1:\n",
        "                    processed_images.append({\n",
        "                        'image_id': data['image_id'],\n",
        "                        'filename': data['filename']\n",
        "                    })\n",
        "\n",
        "            processed_count += len(batch_data)\n",
        "\n",
        "            # Clean up\n",
        "            for data in batch_data:\n",
        "                if hasattr(data['image'], 'close'):\n",
        "                    data['image'].close()\n",
        "\n",
        "            # Save checkpoint\n",
        "            if processed_count % save_interval == 0:\n",
        "                # Convert to final format for checkpoint\n",
        "                checkpoint_results = []\n",
        "                for img_id in sorted(detections_by_image.keys()):\n",
        "                    checkpoint_results.extend(detections_by_image[img_id])\n",
        "\n",
        "                checkpoint_data = {\n",
        "                    'results': checkpoint_results,\n",
        "                    'processed_count': processed_count,\n",
        "                    'detection_count': len(checkpoint_results)\n",
        "                }\n",
        "\n",
        "                # Save checkpoint with compact arrays\n",
        "                def save_compact_checkpoint(data, filename):\n",
        "                    import json, re\n",
        "                    json_str = json.dumps(data, indent=2)\n",
        "                    array_pattern = r'\\[\\s*(\\d+(?:\\.\\d+)?(?:\\s*,\\s*\\d+(?:\\.\\d+)?)*)\\s*\\]'\n",
        "                    def compact_array(match):\n",
        "                        numbers = re.sub(r'\\s*,\\s*', ', ', match.group(1))\n",
        "                        return f'[{numbers}]'\n",
        "                    json_str = re.sub(array_pattern, compact_array, json_str)\n",
        "                    with open(filename, 'w') as f:\n",
        "                        f.write(json_str)\n",
        "\n",
        "                checkpoint_file = f\"{output_file}.checkpoint_{processed_count}.json\"\n",
        "                save_compact_checkpoint(checkpoint_data, checkpoint_file)\n",
        "\n",
        "            # Memory cleanup\n",
        "            if batch_idx % 50 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    # Create final output - all images in order (with and without detections)\n",
        "    final_results = []\n",
        "\n",
        "    # Get all processed images sorted by image_id\n",
        "    all_processed_images = sorted(processed_images, key=lambda x: x['image_id'])\n",
        "\n",
        "    for img_info in all_processed_images:\n",
        "        img_id = img_info['image_id']\n",
        "\n",
        "        if img_id in detections_by_image:\n",
        "            # Add all detections for this image\n",
        "            final_results.extend(detections_by_image[img_id])\n",
        "        else:\n",
        "            # Add entry for image with no detections\n",
        "            final_results.append({\n",
        "                'image_id': img_id,\n",
        "                'detections': []  # Empty list indicates no detections\n",
        "            })\n",
        "\n",
        "    final_output = {\n",
        "        'results': final_results,  # All images in order by image_id\n",
        "        'metadata': {\n",
        "            'total_images_processed': processed_count,\n",
        "            'total_detections': sum(1 for r in final_results if 'person_box' in r),\n",
        "            'images_with_detections': len(detections_by_image),\n",
        "            'images_without_detections': processed_count - len(detections_by_image),\n",
        "            'detection_rate': f\"{len(detections_by_image)/processed_count*100:.1f}%\" if processed_count > 0 else \"0%\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save final results with arrays on one line\n",
        "    def save_compact_json(data, filename):\n",
        "        \"\"\"Save JSON with arrays on one line\"\"\"\n",
        "        import json\n",
        "\n",
        "        # Convert to JSON string with normal formatting\n",
        "        json_str = json.dumps(data, indent=2)\n",
        "\n",
        "        # Fix array formatting to be on one line\n",
        "        import re\n",
        "\n",
        "        # Pattern to match arrays with numbers\n",
        "        array_pattern = r'\\[\\s*(\\d+(?:\\.\\d+)?(?:\\s*,\\s*\\d+(?:\\.\\d+)?)*)\\s*\\]'\n",
        "\n",
        "        def compact_array(match):\n",
        "            # Extract numbers and put them on one line\n",
        "            numbers = match.group(1)\n",
        "            # Clean up spacing\n",
        "            numbers = re.sub(r'\\s*,\\s*', ', ', numbers)\n",
        "            return f'[{numbers}]'\n",
        "\n",
        "        # Apply the pattern\n",
        "        json_str = re.sub(array_pattern, compact_array, json_str)\n",
        "\n",
        "        # Write to file\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(json_str)\n",
        "\n",
        "    save_compact_json(final_output, output_file)\n",
        "\n",
        "    return final_results"
      ],
      "metadata": {
        "id": "wRA06ElmsP4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Quick Test\n",
        "def run_fast_test():\n",
        "    \"\"\"Quick test on small batch\"\"\"\n",
        "    model, processor = init_model_fast()\n",
        "\n",
        "    results = process_coco_fast(\n",
        "        model,\n",
        "        processor,\n",
        "        output_file=\"fast_test_results.json\",\n",
        "        batch_size=16,\n",
        "        max_images=50,\n",
        "        save_interval=25\n",
        "    )\n",
        "\n",
        "    if results:\n",
        "        for i, result in enumerate(results[:3]):\n",
        "            json.dumps(result, indent=2)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "t5eXWfz_sSYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Full Fast Processing\n",
        "def run_full_fast():\n",
        "    \"\"\"Full dataset processing optimized for speed\"\"\"\n",
        "    model, processor = init_model_fast()\n",
        "\n",
        "    # A100 optimized settings\n",
        "    batch_size = 28  # Large batch for A100\n",
        "\n",
        "    results = process_coco_fast(\n",
        "        model,\n",
        "        processor,\n",
        "        output_file=\"coco_hoi_full_fast.json\",\n",
        "        batch_size=batch_size,\n",
        "        max_images=None,  # All images\n",
        "        save_interval=2000\n",
        "    )\n",
        "\n",
        "    # Quick analysis\n",
        "    if results:\n",
        "        actions = {}\n",
        "        objects = {}\n",
        "        for result in results:\n",
        "            for key in result.keys():\n",
        "                if key.endswith('_agent'):\n",
        "                    action = key.replace('_agent', '')\n",
        "                    actions[action] = actions.get(action, 0) + 1\n",
        "                elif '_' in key and key not in ['image_id', 'person_box']:\n",
        "                    parts = key.split('_', 1)\n",
        "                    if len(parts) == 2:\n",
        "                        obj = parts[1]\n",
        "                        objects[obj] = objects.get(obj, 0) + 1\n",
        "\n",
        "    return results\n",
        "\n",
        "# Uncomment to run full processing\n",
        "full_results = run_full_fast()"
      ],
      "metadata": {
        "id": "_64UTkdZrzJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOISBUh8sd4fqPEmgt/0diY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}